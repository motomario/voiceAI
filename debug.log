INFO 2023-12-17 19:13:21,673 autoreload Watching for file changes with StatReloader
DEBUG 2023-12-17 19:13:21,897 _config load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG 2023-12-17 19:13:21,898 _config load_verify_locations cafile='/Users/marius/PycharmProjects/voiceAI/venv/lib/python3.12/site-packages/certifi/cacert.pem'
WARNING 2023-12-17 19:13:25,897 log Not Found: /cart.json
DEBUG 2023-12-17 19:13:29,390 views Received transcript: Mississippi test
DEBUG 2023-12-17 19:13:29,391 views Current conversation history: []
DEBUG 2023-12-17 19:13:29,391 views Updated conversation history: [{'role': 'user', 'content': 'Mississippi test'}]
DEBUG 2023-12-17 19:13:29,399 _base_client Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'Mississippi test'}], 'model': 'gpt-3.5-turbo'}}
DEBUG 2023-12-17 19:13:29,448 _trace connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG 2023-12-17 19:13:29,606 _trace connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x103c0fc20>
DEBUG 2023-12-17 19:13:29,607 _trace start_tls.started ssl_context=<ssl.SSLContext object at 0x10385b8d0> server_hostname='api.openai.com' timeout=5.0
DEBUG 2023-12-17 19:13:29,625 _trace start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x103c70740>
DEBUG 2023-12-17 19:13:29,625 _trace send_request_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:13:29,626 _trace send_request_headers.complete
DEBUG 2023-12-17 19:13:29,626 _trace send_request_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:13:29,626 _trace send_request_body.complete
DEBUG 2023-12-17 19:13:29,626 _trace receive_response_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:13:35,198 _trace receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 17 Dec 2023 19:13:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-ligkrnmiuilousymxxhyfgkv'), (b'openai-processing-ms', b'5345'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'60000'), (b'x-ratelimit-limit-tokens_usage_based', b'60000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'59979'), (b'x-ratelimit-remaining-tokens_usage_based', b'59979'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'21ms'), (b'x-ratelimit-reset-tokens_usage_based', b'21ms'), (b'x-request-id', b'e0088f7c48d6cbffb57455dd899a7455'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=uw3ZszT78kxS33rEgnLHEKrcj37BPO2qgTlDAMF.Aow-1702840415-1-AfOaYF0cUV5nCWQpBw+UTAweJQknA1vXWsOG5kOsa0uN86ZXfNi8Q+IHFuYC7R5qAaCfaICIZacg1qqtVuekvrg=; path=/; expires=Sun, 17-Dec-23 19:43:35 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=1mcGFE4kHJmlMo.v.1.2WRotgezTbA0bUnXT365Fl70-1702840415255-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83717bd0986cc035-VNO'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO 2023-12-17 19:13:35,200 _client HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG 2023-12-17 19:13:35,200 _trace receive_response_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:13:35,200 _trace receive_response_body.complete
DEBUG 2023-12-17 19:13:35,200 _trace response_closed.started
DEBUG 2023-12-17 19:13:35,200 _trace response_closed.complete
DEBUG 2023-12-17 19:13:35,201 _base_client HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
ERROR 2023-12-17 19:13:35,206 views Error processing the command
Traceback (most recent call last):
  File "/Users/marius/PycharmProjects/voiceAI/voiceAI/views.py", line 44, in process_command
    ai_response = response.choices[0].message['content']
                  ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
TypeError: 'ChatCompletionMessage' object is not subscriptable
INFO 2023-12-17 19:21:31,387 autoreload /Users/marius/PycharmProjects/voiceAI/voiceAI/views.py changed, reloading.
INFO 2023-12-17 19:21:31,843 autoreload Watching for file changes with StatReloader
DEBUG 2023-12-17 19:21:32,091 _config load_ssl_context verify=True cert=None trust_env=True http2=False
DEBUG 2023-12-17 19:21:32,093 _config load_verify_locations cafile='/Users/marius/PycharmProjects/voiceAI/venv/lib/python3.12/site-packages/certifi/cacert.pem'
DEBUG 2023-12-17 19:21:38,660 views Received transcript: this is a test
DEBUG 2023-12-17 19:21:38,660 views Current conversation history: []
DEBUG 2023-12-17 19:21:38,660 views Updated conversation history: [{'role': 'user', 'content': 'this is a test'}]
DEBUG 2023-12-17 19:21:38,664 _base_client Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'this is a test'}], 'model': 'gpt-3.5-turbo'}}
DEBUG 2023-12-17 19:21:38,695 _trace connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG 2023-12-17 19:21:38,864 _trace connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10441d400>
DEBUG 2023-12-17 19:21:38,864 _trace start_tls.started ssl_context=<ssl.SSLContext object at 0x10400b8d0> server_hostname='api.openai.com' timeout=5.0
DEBUG 2023-12-17 19:21:38,884 _trace start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10403f350>
DEBUG 2023-12-17 19:21:38,885 _trace send_request_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:38,885 _trace send_request_headers.complete
DEBUG 2023-12-17 19:21:38,885 _trace send_request_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:38,886 _trace send_request_body.complete
DEBUG 2023-12-17 19:21:38,886 _trace receive_response_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:39,692 _trace receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 17 Dec 2023 19:21:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-ligkrnmiuilousymxxhyfgkv'), (b'openai-processing-ms', b'587'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'60000'), (b'x-ratelimit-limit-tokens_usage_based', b'60000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'59979'), (b'x-ratelimit-remaining-tokens_usage_based', b'59979'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'21ms'), (b'x-ratelimit-reset-tokens_usage_based', b'21ms'), (b'x-request-id', b'e83bbe81faeafccfe5a6feb0e91c6a23'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=ONKk5.ICpGwMCGFcQdB2nVwa963H8MJuAARd9r3Pa50-1702840899-1-AQ3mr+ymEzHAOxCIX6fijms+vCMYB+Dp8kiHseQkGSmXVAsdW5XAWvvNq7c9mIPB9vfW9nMTxGVMfyJjXzV+Ga0=; path=/; expires=Sun, 17-Dec-23 19:51:39 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=HAen7EgsjyIEb8vWD30wj40Yyx3vQpvgSHaoQ.ho0_4-1702840899747-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'837187c27d2cffec-VNO'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO 2023-12-17 19:21:39,695 _client HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG 2023-12-17 19:21:39,695 _trace receive_response_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:39,695 _trace receive_response_body.complete
DEBUG 2023-12-17 19:21:39,695 _trace response_closed.started
DEBUG 2023-12-17 19:21:39,695 _trace response_closed.complete
DEBUG 2023-12-17 19:21:39,696 _base_client HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
DEBUG 2023-12-17 19:21:46,160 views Received transcript:  tell me a joke about a Visa
DEBUG 2023-12-17 19:21:46,160 views Current conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}]
DEBUG 2023-12-17 19:21:46,160 views Updated conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}]
DEBUG 2023-12-17 19:21:46,168 _base_client Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}], 'model': 'gpt-3.5-turbo'}}
DEBUG 2023-12-17 19:21:46,170 _trace close.started
DEBUG 2023-12-17 19:21:46,171 _trace close.complete
DEBUG 2023-12-17 19:21:46,171 _trace connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG 2023-12-17 19:21:46,183 _trace connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x104381d90>
DEBUG 2023-12-17 19:21:46,184 _trace start_tls.started ssl_context=<ssl.SSLContext object at 0x10400b8d0> server_hostname='api.openai.com' timeout=5.0
DEBUG 2023-12-17 19:21:46,199 _trace start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x104380740>
DEBUG 2023-12-17 19:21:46,199 _trace send_request_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:46,200 _trace send_request_headers.complete
DEBUG 2023-12-17 19:21:46,200 _trace send_request_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:46,200 _trace send_request_body.complete
DEBUG 2023-12-17 19:21:46,200 _trace receive_response_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:47,751 _trace receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 17 Dec 2023 19:21:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-ligkrnmiuilousymxxhyfgkv'), (b'openai-processing-ms', b'1302'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'60000'), (b'x-ratelimit-limit-tokens_usage_based', b'60000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'59961'), (b'x-ratelimit-remaining-tokens_usage_based', b'59961'), (b'x-ratelimit-reset-requests', b'9.931s'), (b'x-ratelimit-reset-tokens', b'39ms'), (b'x-ratelimit-reset-tokens_usage_based', b'39ms'), (b'x-request-id', b'f4670f47711307e04597294864aef7ab'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'837187f02aa6c030-VNO'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO 2023-12-17 19:21:47,752 _client HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG 2023-12-17 19:21:47,752 _trace receive_response_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:47,752 _trace receive_response_body.complete
DEBUG 2023-12-17 19:21:47,752 _trace response_closed.started
DEBUG 2023-12-17 19:21:47,753 _trace response_closed.complete
DEBUG 2023-12-17 19:21:47,753 _base_client HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
DEBUG 2023-12-17 19:21:57,566 views Received transcript:  tell me another joke about the dog
DEBUG 2023-12-17 19:21:57,566 views Current conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}]
DEBUG 2023-12-17 19:21:57,566 views Updated conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}]
DEBUG 2023-12-17 19:21:57,573 _base_client Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}], 'model': 'gpt-3.5-turbo'}}
DEBUG 2023-12-17 19:21:57,574 _trace close.started
DEBUG 2023-12-17 19:21:57,574 _trace close.complete
DEBUG 2023-12-17 19:21:57,574 _trace connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG 2023-12-17 19:21:57,586 _trace connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x104382480>
DEBUG 2023-12-17 19:21:57,586 _trace start_tls.started ssl_context=<ssl.SSLContext object at 0x10400b8d0> server_hostname='api.openai.com' timeout=5.0
DEBUG 2023-12-17 19:21:57,601 _trace start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1043823f0>
DEBUG 2023-12-17 19:21:57,601 _trace send_request_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:57,601 _trace send_request_headers.complete
DEBUG 2023-12-17 19:21:57,602 _trace send_request_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:57,602 _trace send_request_body.complete
DEBUG 2023-12-17 19:21:57,602 _trace receive_response_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:58,816 _trace receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 17 Dec 2023 19:21:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-ligkrnmiuilousymxxhyfgkv'), (b'openai-processing-ms', b'894'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'60000'), (b'x-ratelimit-limit-tokens_usage_based', b'60000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'59925'), (b'x-ratelimit-remaining-tokens_usage_based', b'59925'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'75ms'), (b'x-ratelimit-reset-tokens_usage_based', b'75ms'), (b'x-request-id', b'671266a1cbc77dd2680724b5c66c00cf'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'837188376d7dbc1c-VNO'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO 2023-12-17 19:21:58,817 _client HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG 2023-12-17 19:21:58,817 _trace receive_response_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:21:58,818 _trace receive_response_body.complete
DEBUG 2023-12-17 19:21:58,818 _trace response_closed.started
DEBUG 2023-12-17 19:21:58,818 _trace response_closed.complete
DEBUG 2023-12-17 19:21:58,818 _base_client HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
DEBUG 2023-12-17 19:22:04,261 views Received transcript:  tell me a joke about a horse
DEBUG 2023-12-17 19:22:04,261 views Current conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}]
DEBUG 2023-12-17 19:22:04,261 views Updated conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}]
DEBUG 2023-12-17 19:22:04,272 _base_client Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}], 'model': 'gpt-3.5-turbo'}}
DEBUG 2023-12-17 19:22:04,274 _trace close.started
DEBUG 2023-12-17 19:22:04,274 _trace close.complete
DEBUG 2023-12-17 19:22:04,274 _trace connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG 2023-12-17 19:22:04,286 _trace connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1043bdc40>
DEBUG 2023-12-17 19:22:04,287 _trace start_tls.started ssl_context=<ssl.SSLContext object at 0x10400b8d0> server_hostname='api.openai.com' timeout=5.0
DEBUG 2023-12-17 19:22:04,302 _trace start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1043be060>
DEBUG 2023-12-17 19:22:04,302 _trace send_request_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:04,303 _trace send_request_headers.complete
DEBUG 2023-12-17 19:22:04,303 _trace send_request_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:04,303 _trace send_request_body.complete
DEBUG 2023-12-17 19:22:04,303 _trace receive_response_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:05,814 _trace receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 17 Dec 2023 19:22:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-ligkrnmiuilousymxxhyfgkv'), (b'openai-processing-ms', b'1187'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'60000'), (b'x-ratelimit-limit-tokens_usage_based', b'60000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'59897'), (b'x-ratelimit-remaining-tokens_usage_based', b'59897'), (b'x-ratelimit-reset-requests', b'10.592s'), (b'x-ratelimit-reset-tokens', b'103ms'), (b'x-ratelimit-reset-tokens_usage_based', b'103ms'), (b'x-request-id', b'870f8953b18879d8ed7932446406bc8a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'837188615d52c03f-VNO'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO 2023-12-17 19:22:05,816 _client HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG 2023-12-17 19:22:05,816 _trace receive_response_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:05,817 _trace receive_response_body.complete
DEBUG 2023-12-17 19:22:05,817 _trace response_closed.started
DEBUG 2023-12-17 19:22:05,817 _trace response_closed.complete
DEBUG 2023-12-17 19:22:05,817 _base_client HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
DEBUG 2023-12-17 19:22:12,510 views Received transcript:  what was the first joke about
DEBUG 2023-12-17 19:22:12,510 views Current conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}]
DEBUG 2023-12-17 19:22:12,510 views Updated conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}]
DEBUG 2023-12-17 19:22:12,522 _base_client Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}], 'model': 'gpt-3.5-turbo'}}
DEBUG 2023-12-17 19:22:12,523 _trace close.started
DEBUG 2023-12-17 19:22:12,523 _trace close.complete
DEBUG 2023-12-17 19:22:12,523 _trace connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG 2023-12-17 19:22:12,536 _trace connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1043809b0>
DEBUG 2023-12-17 19:22:12,536 _trace start_tls.started ssl_context=<ssl.SSLContext object at 0x10400b8d0> server_hostname='api.openai.com' timeout=5.0
DEBUG 2023-12-17 19:22:12,553 _trace start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x104383b60>
DEBUG 2023-12-17 19:22:12,554 _trace send_request_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:12,554 _trace send_request_headers.complete
DEBUG 2023-12-17 19:22:12,554 _trace send_request_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:12,554 _trace send_request_body.complete
DEBUG 2023-12-17 19:22:12,554 _trace receive_response_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:13,608 _trace receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 17 Dec 2023 19:22:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-ligkrnmiuilousymxxhyfgkv'), (b'openai-processing-ms', b'841'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'60000'), (b'x-ratelimit-limit-tokens_usage_based', b'60000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'59859'), (b'x-ratelimit-remaining-tokens_usage_based', b'59859'), (b'x-ratelimit-reset-requests', b'10.984s'), (b'x-ratelimit-reset-tokens', b'141ms'), (b'x-ratelimit-reset-tokens_usage_based', b'141ms'), (b'x-request-id', b'b1d4d2a984cd4ec6e52cb844dda8ca4b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83718894e87abc19-VNO'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO 2023-12-17 19:22:13,608 _client HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG 2023-12-17 19:22:13,608 _trace receive_response_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:13,609 _trace receive_response_body.complete
DEBUG 2023-12-17 19:22:13,609 _trace response_closed.started
DEBUG 2023-12-17 19:22:13,609 _trace response_closed.complete
DEBUG 2023-12-17 19:22:13,609 _base_client HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
DEBUG 2023-12-17 19:22:18,908 views Received transcript:  what was the second joke about
DEBUG 2023-12-17 19:22:18,909 views Current conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}]
DEBUG 2023-12-17 19:22:18,909 views Updated conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}]
DEBUG 2023-12-17 19:22:18,926 _base_client Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}], 'model': 'gpt-3.5-turbo'}}
DEBUG 2023-12-17 19:22:18,927 _trace close.started
DEBUG 2023-12-17 19:22:18,928 _trace close.complete
DEBUG 2023-12-17 19:22:18,928 _trace connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG 2023-12-17 19:22:18,939 _trace connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10403f350>
DEBUG 2023-12-17 19:22:18,939 _trace start_tls.started ssl_context=<ssl.SSLContext object at 0x10400b8d0> server_hostname='api.openai.com' timeout=5.0
DEBUG 2023-12-17 19:22:18,957 _trace start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1043ca6f0>
DEBUG 2023-12-17 19:22:18,958 _trace send_request_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:18,958 _trace send_request_headers.complete
DEBUG 2023-12-17 19:22:18,958 _trace send_request_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:18,959 _trace send_request_body.complete
DEBUG 2023-12-17 19:22:18,959 _trace receive_response_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:19,721 _trace receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 17 Dec 2023 19:22:19 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-ligkrnmiuilousymxxhyfgkv'), (b'openai-processing-ms', b'435'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'60000'), (b'x-ratelimit-limit-tokens_usage_based', b'60000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'59837'), (b'x-ratelimit-remaining-tokens_usage_based', b'59837'), (b'x-ratelimit-reset-requests', b'13.214s'), (b'x-ratelimit-reset-tokens', b'163ms'), (b'x-ratelimit-reset-tokens_usage_based', b'163ms'), (b'x-request-id', b'04501cd7fb62845a05aad562900a93a4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'837188bcea73bc16-VNO'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO 2023-12-17 19:22:19,721 _client HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG 2023-12-17 19:22:19,722 _trace receive_response_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:19,722 _trace receive_response_body.complete
DEBUG 2023-12-17 19:22:19,722 _trace response_closed.started
DEBUG 2023-12-17 19:22:19,723 _trace response_closed.complete
DEBUG 2023-12-17 19:22:19,723 _base_client HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
DEBUG 2023-12-17 19:22:25,993 views Received transcript:  what was the first joke about
DEBUG 2023-12-17 19:22:25,993 views Current conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}, {'role': 'assistant', 'content': 'The second joke was about a dog!'}]
DEBUG 2023-12-17 19:22:25,993 views Updated conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}, {'role': 'assistant', 'content': 'The second joke was about a dog!'}, {'role': 'user', 'content': ' what was the first joke about'}]
DEBUG 2023-12-17 19:22:26,013 _base_client Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}, {'role': 'assistant', 'content': 'The second joke was about a dog!'}, {'role': 'user', 'content': ' what was the first joke about'}], 'model': 'gpt-3.5-turbo'}}
DEBUG 2023-12-17 19:22:26,014 _trace close.started
DEBUG 2023-12-17 19:22:26,015 _trace close.complete
DEBUG 2023-12-17 19:22:26,015 _trace connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG 2023-12-17 19:22:26,027 _trace connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1043ca690>
DEBUG 2023-12-17 19:22:26,028 _trace start_tls.started ssl_context=<ssl.SSLContext object at 0x10400b8d0> server_hostname='api.openai.com' timeout=5.0
DEBUG 2023-12-17 19:22:26,041 _trace start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1043c81a0>
DEBUG 2023-12-17 19:22:26,041 _trace send_request_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:26,042 _trace send_request_headers.complete
DEBUG 2023-12-17 19:22:26,042 _trace send_request_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:26,042 _trace send_request_body.complete
DEBUG 2023-12-17 19:22:26,042 _trace receive_response_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:26,905 _trace receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 17 Dec 2023 19:22:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-ligkrnmiuilousymxxhyfgkv'), (b'openai-processing-ms', b'639'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'60000'), (b'x-ratelimit-limit-tokens_usage_based', b'60000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'59820'), (b'x-ratelimit-remaining-tokens_usage_based', b'59820'), (b'x-ratelimit-reset-requests', b'14.767s'), (b'x-ratelimit-reset-tokens', b'180ms'), (b'x-ratelimit-reset-tokens_usage_based', b'180ms'), (b'x-request-id', b'c637815eb846f8865cfacfa07dbc08c5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'837188e93e20c03f-VNO'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO 2023-12-17 19:22:26,905 _client HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG 2023-12-17 19:22:26,905 _trace receive_response_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:26,906 _trace receive_response_body.complete
DEBUG 2023-12-17 19:22:26,906 _trace response_closed.started
DEBUG 2023-12-17 19:22:26,906 _trace response_closed.complete
DEBUG 2023-12-17 19:22:26,906 _base_client HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
DEBUG 2023-12-17 19:22:34,515 views Received transcript:  this conversation
DEBUG 2023-12-17 19:22:34,515 views Current conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}, {'role': 'assistant', 'content': 'The second joke was about a dog!'}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'Apologies for the confusion earlier. The first joke was about a Visa.'}]
DEBUG 2023-12-17 19:22:34,515 views Updated conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}, {'role': 'assistant', 'content': 'The second joke was about a dog!'}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'Apologies for the confusion earlier. The first joke was about a Visa.'}, {'role': 'user', 'content': ' this conversation'}]
DEBUG 2023-12-17 19:22:34,539 _base_client Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}, {'role': 'assistant', 'content': 'The second joke was about a dog!'}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'Apologies for the confusion earlier. The first joke was about a Visa.'}, {'role': 'user', 'content': ' this conversation'}], 'model': 'gpt-3.5-turbo'}}
DEBUG 2023-12-17 19:22:34,546 _trace close.started
DEBUG 2023-12-17 19:22:34,547 _trace close.complete
DEBUG 2023-12-17 19:22:34,547 _trace connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG 2023-12-17 19:22:34,562 _trace connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1043f3380>
DEBUG 2023-12-17 19:22:34,563 _trace start_tls.started ssl_context=<ssl.SSLContext object at 0x10400b8d0> server_hostname='api.openai.com' timeout=5.0
DEBUG 2023-12-17 19:22:34,581 _trace start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1043f3530>
DEBUG 2023-12-17 19:22:34,582 _trace send_request_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:34,582 _trace send_request_headers.complete
DEBUG 2023-12-17 19:22:34,582 _trace send_request_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:34,582 _trace send_request_body.complete
DEBUG 2023-12-17 19:22:34,583 _trace receive_response_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:36,239 _trace receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 17 Dec 2023 19:22:36 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-ligkrnmiuilousymxxhyfgkv'), (b'openai-processing-ms', b'1412'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'60000'), (b'x-ratelimit-limit-tokens_usage_based', b'60000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'59796'), (b'x-ratelimit-remaining-tokens_usage_based', b'59796'), (b'x-ratelimit-reset-requests', b'14.856s'), (b'x-ratelimit-reset-tokens', b'204ms'), (b'x-ratelimit-reset-tokens_usage_based', b'204ms'), (b'x-request-id', b'9602c3a7263231bfcb0b5bafa78c5f1a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8371891e9b27bc1b-VNO'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO 2023-12-17 19:22:36,240 _client HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG 2023-12-17 19:22:36,240 _trace receive_response_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:36,241 _trace receive_response_body.complete
DEBUG 2023-12-17 19:22:36,241 _trace response_closed.started
DEBUG 2023-12-17 19:22:36,241 _trace response_closed.complete
DEBUG 2023-12-17 19:22:36,241 _base_client HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
DEBUG 2023-12-17 19:22:48,770 views Received transcript:  yes Yahoo
DEBUG 2023-12-17 19:22:48,770 views Current conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}, {'role': 'assistant', 'content': 'The second joke was about a dog!'}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'Apologies for the confusion earlier. The first joke was about a Visa.'}, {'role': 'user', 'content': ' this conversation'}, {'role': 'assistant', 'content': "This conversation has been a mixture of testing and sharing jokes. Is there anything else I can assist you with or any topic you'd like to discuss?"}]
DEBUG 2023-12-17 19:22:48,771 views Updated conversation history: [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}, {'role': 'assistant', 'content': 'The second joke was about a dog!'}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'Apologies for the confusion earlier. The first joke was about a Visa.'}, {'role': 'user', 'content': ' this conversation'}, {'role': 'assistant', 'content': "This conversation has been a mixture of testing and sharing jokes. Is there anything else I can assist you with or any topic you'd like to discuss?"}, {'role': 'user', 'content': ' yes Yahoo'}]
DEBUG 2023-12-17 19:22:48,791 _base_client Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'this is a test'}, {'role': 'assistant', 'content': 'Great! What would you like to test?'}, {'role': 'user', 'content': ' tell me a joke about a Visa'}, {'role': 'assistant', 'content': "Sure, here's a joke about a Visa:\n\nWhy did the credit card go to therapy?\n\nBecause it had Visa issues!"}, {'role': 'user', 'content': ' tell me another joke about the dog'}, {'role': 'assistant', 'content': "Why did the dog sit in the shade?\n\nBecause it didn't want to be a hot dog!"}, {'role': 'user', 'content': ' tell me a joke about a horse'}, {'role': 'assistant', 'content': "Sure, here's a joke about a horse:\n\nWhy did the horse start a band?\n\nBecause it already had plenty of neigh-sayers!"}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'The first joke was about a Visa, the credit card!'}, {'role': 'user', 'content': ' what was the second joke about'}, {'role': 'assistant', 'content': 'The second joke was about a dog!'}, {'role': 'user', 'content': ' what was the first joke about'}, {'role': 'assistant', 'content': 'Apologies for the confusion earlier. The first joke was about a Visa.'}, {'role': 'user', 'content': ' this conversation'}, {'role': 'assistant', 'content': "This conversation has been a mixture of testing and sharing jokes. Is there anything else I can assist you with or any topic you'd like to discuss?"}, {'role': 'user', 'content': ' yes Yahoo'}], 'model': 'gpt-3.5-turbo'}}
DEBUG 2023-12-17 19:22:48,791 _trace close.started
DEBUG 2023-12-17 19:22:48,791 _trace close.complete
DEBUG 2023-12-17 19:22:48,791 _trace connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
DEBUG 2023-12-17 19:22:48,803 _trace connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x104383470>
DEBUG 2023-12-17 19:22:48,803 _trace start_tls.started ssl_context=<ssl.SSLContext object at 0x10400b8d0> server_hostname='api.openai.com' timeout=5.0
DEBUG 2023-12-17 19:22:48,817 _trace start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x104381fd0>
DEBUG 2023-12-17 19:22:48,818 _trace send_request_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:48,818 _trace send_request_headers.complete
DEBUG 2023-12-17 19:22:48,818 _trace send_request_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:48,818 _trace send_request_body.complete
DEBUG 2023-12-17 19:22:48,818 _trace receive_response_headers.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:52,960 _trace receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 17 Dec 2023 19:22:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'user-ligkrnmiuilousymxxhyfgkv'), (b'openai-processing-ms', b'3022'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'60000'), (b'x-ratelimit-limit-tokens_usage_based', b'60000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'59755'), (b'x-ratelimit-remaining-tokens_usage_based', b'59755'), (b'x-ratelimit-reset-requests', b'9.273s'), (b'x-ratelimit-reset-tokens', b'245ms'), (b'x-ratelimit-reset-tokens_usage_based', b'245ms'), (b'x-request-id', b'555d2d5d2ab2ea5ebbbe4902ea8e0bb4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'837189778d16c035-VNO'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
INFO 2023-12-17 19:22:52,962 _client HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG 2023-12-17 19:22:52,962 _trace receive_response_body.started request=<Request [b'POST']>
DEBUG 2023-12-17 19:22:52,963 _trace receive_response_body.complete
DEBUG 2023-12-17 19:22:52,964 _trace response_closed.started
DEBUG 2023-12-17 19:22:52,964 _trace response_closed.complete
DEBUG 2023-12-17 19:22:52,964 _base_client HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
